# Model Evaluation: From Beginner to Expert

## ğŸ“š Table of Contents

- [Introduction](#introduction)
- [Part I: Beginner Level](#part-i-beginner-level)
  - [Chapter 1: Why Model Evaluation Matters](#chapter-1-why-model-evaluation-matters)
  - [Chapter 2: Regression Metrics Fundamentals](#chapter-2-regression-metrics-fundamentals)
  - [Chapter 3: Classification Metrics Fundamentals](#chapter-3-classification-metrics-fundamentals)
  - [Chapter 4: The Train-Test Split](#chapter-4-the-train-test-split)
- [Part II: Intermediate Level](#part-ii-intermediate-level)
  - [Chapter 5: Understanding RÂ² (Coefficient of Determination)](#chapter-5-understanding-rÂ²-coefficient-of-determination)
  - [Chapter 6: The Confusion Matrix](#chapter-6-the-confusion-matrix)
  - [Chapter 7: Precision-Recall Tradeoff](#chapter-7-precision-recall-tradeoff)
  - [Chapter 8: ROC Curves and AUC](#chapter-8-roc-curves-and-auc)
- [Part III: Advanced Level](#part-iii-advanced-level)
  - [Chapter 9: Cross-Validation Techniques](#chapter-9-cross-validation-techniques)
  - [Chapter 10: Learning Curves](#chapter-10-learning-curves)
  - [Chapter 11: Model Selection and Comparison](#chapter-11-model-selection-and-comparison)
  - [Chapter 12: Production Monitoring](#chapter-12-production-monitoring)

---

## Introduction

Welcome to the comprehensive guide on **Model Evaluation**! This tutorial will teach you how to properly assess machine learning models, from basic metrics to advanced validation techniques.

### What You'll Learn

| Level | Duration | Topics Covered | Skills Acquired |
|-------|----------|----------------|-----------------|
| **Beginner** | 1-2 weeks | Basic metrics, Train-test split | Calculate and interpret basic metrics |
| **Intermediate** | 2-4 weeks | Advanced metrics, Confusion matrix, ROC curves | Choose appropriate metrics, analyze results |
| **Advanced** | 3-5 weeks | Cross-validation, Learning curves, Model selection | Robust evaluation, diagnose problems |

### Prerequisites

- Understanding of supervised learning
- Familiarity with regression and classification
- Basic C# programming
- Basic statistics knowledge

### The Golden Rule of Evaluation

> **Never evaluate your model on the training data!**

This is the most important rule in machine learning evaluation. Always use independent test data to assess generalization performance.

---

## Part I: Beginner Level

### Chapter 1: Why Model Evaluation Matters

#### 1.1 The Purpose of Evaluation

**Model evaluation serves four critical purposes**:

1. **Measure Performance**: Quantify how well your model works
2. **Compare Models**: Decide which algorithm performs best
3. **Detect Problems**: Identify overfitting or underfitting
4. **Guide Optimization**: Know what to improve

#### 1.2 The Evaluation Workflow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  1. Train Model â”‚
â”‚   on Train Set  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  2. Predict on  â”‚
â”‚    Test Set     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  3. Calculate   â”‚
â”‚     Metrics     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  4. Interpret   â”‚
â”‚     Results     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  5. Make        â”‚
â”‚   Decisions     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### 1.3 Common Evaluation Mistakes

**Mistake 1: Evaluating on Training Data**
```
âŒ BAD:
model.Fit(XTrain, yTrain);
predictions = model.Predict(XTrain);  // Using training data!
accuracy = CalculateAccuracy(yTrain, predictions);
// Result: Overly optimistic, doesn't measure generalization

âœ… GOOD:
model.Fit(XTrain, yTrain);
predictions = model.Predict(XTest);   // Using test data!
accuracy = CalculateAccuracy(yTest, predictions);
// Result: Realistic measure of generalization
```

**Mistake 2: Data Leakage**
```
âŒ BAD:
// Scale all data together
XScaled = StandardizeFeatures(X);
// Then split
(XTrain, XTest) = Split(XScaled);
// Problem: Test data influenced training statistics!

âœ… GOOD:
// Split first
(XTrain, XTest) = Split(X);
// Scale separately
XTrain = StandardizeFeatures(XTrain);
XTest = StandardizeFeatures(XTest);  // Use training statistics
```

**Mistake 3: Using Wrong Metric**
```
âŒ BAD:
// Imbalanced dataset: 95% class 0, 5% class 1
accuracy = 95%  // Model always predicts class 0!
// Looks good but useless

âœ… GOOD:
// Use appropriate metrics
f1Score = 0.12  // Reveals poor performance
recall = 0.0    // Catches no positive cases
```

#### 1.4 Types of Evaluation Metrics

**For Regression Problems**:
- MSE (Mean Squared Error)
- RMSE (Root Mean Squared Error)
- MAE (Mean Absolute Error)
- RÂ² (Coefficient of Determination)
- MAPE (Mean Absolute Percentage Error)

**For Classification Problems**:
- Accuracy
- Precision
- Recall (Sensitivity)
- F1-Score
- Specificity
- ROC-AUC

---

### Chapter 2: Regression Metrics Fundamentals

#### 2.1 Mean Squared Error (MSE)

**Definition**: Average of squared differences between predictions and actual values

```
MSE = (1/n) Î£(yáµ¢ - Å·áµ¢)Â²

Where:
- n = number of samples
- yáµ¢ = actual value
- Å·áµ¢ = predicted value
```

**Characteristics**:
- **Range**: [0, âˆ)
- **Lower is better**: 0 = perfect predictions
- **Units**: Square of target variable units
- **Sensitivity**: Heavily penalizes large errors

**When to Use**:
- When large errors are particularly bad
- When you want to emphasize outliers
- Standard metric for many algorithms

**Code Example**:

```csharp
using ArtificialIntelligence.MachineLearning.Supervised.Evaluation;

// House price predictions (in thousands)
double[] yTrue = new double[] { 200, 250, 300, 350, 400 };
double[] yPred = new double[] { 210, 240, 310, 340, 420 };

double mse = RegressionMetrics.MeanSquaredError(yTrue, yPred);
Console.WriteLine($"MSE: {mse:F2}");
// Output: MSE: 150.00

// Interpretation: Average squared error is 150 (thousand dollars)Â²
```

**Understanding the Result**:
```
Error breakdown:
Sample 1: (200-210)Â² = 100
Sample 2: (250-240)Â² = 100
Sample 3: (300-310)Â² = 100
Sample 4: (350-340)Â² = 100
Sample 5: (400-420)Â² = 400  â† Large error dominates!

MSE = (100+100+100+100+400)/5 = 150
```

#### 2.2 Root Mean Squared Error (RMSE)

**Definition**: Square root of MSE

```
RMSE = âˆšMSE = âˆš[(1/n) Î£(yáµ¢ - Å·áµ¢)Â²]
```

**Characteristics**:
- **Range**: [0, âˆ)
- **Lower is better**: 0 = perfect predictions
- **Units**: Same as target variable (more interpretable!)
- **Sensitivity**: Still penalizes large errors

**Advantages over MSE**:
- Same units as target variable
- More intuitive interpretation
- Easier to communicate to stakeholders

**Code Example**:

```csharp
double rmse = RegressionMetrics.RootMeanSquaredError(yTrue, yPred);
Console.WriteLine($"RMSE: {rmse:F2}");
// Output: RMSE: 12.25

// Interpretation: On average, predictions are off by $12,250
```

**Comparison**:
```
MSE  = 150 (thousand dollars)Â²  â† Hard to interpret
RMSE = 12.25 thousand dollars   â† Easy to interpret!
```

#### 2.3 Mean Absolute Error (MAE)

**Definition**: Average of absolute differences

```
MAE = (1/n) Î£|yáµ¢ - Å·áµ¢|
```

**Characteristics**:
- **Range**: [0, âˆ)
- **Lower is better**: 0 = perfect predictions
- **Units**: Same as target variable
- **Sensitivity**: Treats all errors equally

**MAE vs RMSE**:

| Aspect | MAE | RMSE |
|--------|-----|------|
| **Outlier sensitivity** | Low | High |
| **Interpretation** | Average error | Weighted average error |
| **Use when** | Outliers are noise | Outliers are important |

**Code Example**:

```csharp
double mae = RegressionMetrics.MeanAbsoluteError(yTrue, yPred);
Console.WriteLine($"MAE: {mae:F2}");
// Output: MAE: 10.00

// Interpretation: On average, predictions are off by $10,000
```

**Visual Comparison**:
```
Errors: [10, 10, 10, 10, 20]

MAE  = (10+10+10+10+20)/5 = 12.0  â† Simple average
RMSE = âˆš[(100+100+100+100+400)/5] = 12.25  â† Weighted toward large error
```

#### 2.4 Complete Regression Evaluation Example

**Problem**: Evaluate a house price prediction model

```csharp
using ArtificialIntelligence.MachineLearning.Supervised.Regression;
using ArtificialIntelligence.MachineLearning.Supervised.Evaluation;

public class RegressionEvaluationExample
{
    public static void Main()
    {
        // Step 1: Prepare training data
        // Features: [size_sqft, bedrooms, age_years]
        double[,] XTrain = new double[,] {
            { 1000, 2, 10 },
            { 1500, 3, 5 },
            { 2000, 3, 2 },
            { 2500, 4, 1 },
            { 3000, 4, 0 }
        };

        // Prices in thousands
        double[] yTrain = new double[] { 200, 300, 400, 500, 600 };

        // Step 2: Train model
        var model = new LinearRegression();
        model.Fit(XTrain, yTrain);

        // Step 3: Prepare test data (NEVER seen during training!)
        double[,] XTest = new double[,] {
            { 1200, 2, 8 },
            { 1800, 3, 4 },
            { 2200, 3, 3 }
        };

        double[] yTest = new double[] { 240, 360, 440 };

        // Step 4: Make predictions
        double[] yPred = model.Predict(XTest);

        // Step 5: Calculate all metrics
        double mse = RegressionMetrics.MeanSquaredError(yTest, yPred);
        double rmse = RegressionMetrics.RootMeanSquaredError(yTest, yPred);
        double mae = RegressionMetrics.MeanAbsoluteError(yTest, yPred);
        double r2 = RegressionMetrics.RSquared(yTest, yPred);

        // Step 6: Display evaluation report
        Console.WriteLine("=== Regression Model Evaluation Report ===\n");
        Console.WriteLine($"MSE:  {mse:F2} (thousand dollars)Â²");
        Console.WriteLine($"RMSE: {rmse:F2} thousand dollars");
        Console.WriteLine($"MAE:  {mae:F2} thousand dollars");
        Console.WriteLine($"RÂ²:   {r2:F4}");

        // Step 7: Interpret results
        Console.WriteLine("\n=== Interpretation ===");
        if (rmse < 20)
            Console.WriteLine("âœ… Good: Predictions within $20k on average");
        else
            Console.WriteLine("âŒ Poor: Large prediction errors");

        if (r2 > 0.9)
            Console.WriteLine("âœ… Excellent: Model explains >90% of variance");
        else if (r2 > 0.7)
            Console.WriteLine("âœ… Good: Model explains >70% of variance");
        else
            Console.WriteLine("âŒ Poor: Model doesn't capture patterns well");
    }
}
```

#### 2.5 Practice Exercise

**Exercise**: Evaluate a temperature prediction model

```csharp
// Actual temperatures (Â°F)
double[] actualTemp = new double[] { 72, 75, 68, 80, 77, 73, 71, 76 };

// Predicted temperatures (Â°F)
double[] predictedTemp = new double[] { 70, 76, 70, 78, 75, 74, 72, 77 };

// TODO:
// 1. Calculate MSE, RMSE, and MAE
// 2. Which metric is most interpretable for temperature?
// 3. Is the model performing well? (Consider: Â±2Â°F is acceptable)
```

**Expected Output**:
```
MSE: ~6.5 Â°FÂ²
RMSE: ~2.5 Â°F
MAE: ~2.0 Â°F
Interpretation: Model is performing well (within acceptable range)
```

---

### Chapter 3: Classification Metrics Fundamentals

#### 3.1 Accuracy

**Definition**: Proportion of correct predictions

```
Accuracy = (Correct Predictions) / (Total Predictions)
         = (TP + TN) / (TP + TN + FP + FN)
```

**Characteristics**:
- **Range**: [0, 1] or [0%, 100%]
- **Higher is better**: 1.0 = perfect classification
- **Intuitive**: Easy to understand and explain
- **Limitation**: Misleading for imbalanced data

**Code Example**:

```csharp
using ArtificialIntelligence.MachineLearning.Supervised.Evaluation;

// Email classification: 0=Normal, 1=Spam
int[] yTrue = new int[] { 0, 1, 1, 0, 1, 0, 1, 1, 0, 0 };
int[] yPred = new int[] { 0, 1, 0, 0, 1, 1, 1, 1, 0, 0 };

double accuracy = ClassificationMetrics.Accuracy(yTrue, yPred);
Console.WriteLine($"Accuracy: {accuracy:P2}");
// Output: Accuracy: 80.00%

// Interpretation: 8 out of 10 predictions were correct
```

**When Accuracy is Misleading**:

```
Example: Fraud Detection
Dataset: 990 legitimate, 10 fraudulent (99% vs 1%)

Naive model: Always predict "legitimate"
Accuracy = 990/1000 = 99% âœ…

But:
- Catches 0 frauds âŒ
- Completely useless! âŒ

Lesson: Don't use accuracy for imbalanced data!
```

#### 3.2 Precision

**Definition**: Of all positive predictions, how many were correct?

```
Precision = TP / (TP + FP)
          = True Positives / Predicted Positives
```

**Characteristics**:
- **Range**: [0, 1] or [0%, 100%]
- **Higher is better**: 1.0 = no false positives
- **Focus**: Minimizing false alarms
- **Question**: "When I predict positive, how often am I right?"

**Code Example**:

```csharp
double precision = ClassificationMetrics.Precision(yTrue, yPred, positiveClass: 1);
Console.WriteLine($"Precision: {precision:P2}");
// Output: Precision: 83.33%

// Interpretation: 83% of emails marked as spam are actually spam
```

**Real-World Example**:

```
Spam Filter:
TP = 40 (correctly identified spam)
FP = 10 (legitimate emails marked as spam)

Precision = 40 / (40 + 10) = 0.80 = 80%

Meaning: 80% of emails in spam folder are actually spam
Problem: 20% are false alarms (legitimate emails blocked!)
```

**When to Prioritize Precision**:
- Spam filtering (don't block legitimate emails)
- Medical procedures (avoid unnecessary treatments)
- Product recommendations (don't annoy users)
- Legal decisions (avoid false accusations)

#### 3.3 Recall (Sensitivity)

**Definition**: Of all actual positives, how many did we catch?

```
Recall = TP / (TP + FN)
       = True Positives / Actual Positives
```

**Characteristics**:
- **Range**: [0, 1] or [0%, 100%]
- **Higher is better**: 1.0 = no false negatives
- **Focus**: Minimizing missed positives
- **Question**: "Of all actual positives, how many did I find?"

**Code Example**:

```csharp
double recall = ClassificationMetrics.Recall(yTrue, yPred, positiveClass: 1);
Console.WriteLine($"Recall: {recall:P2}");
// Output: Recall: 80.00%

// Interpretation: We caught 80% of all spam emails
```

**Real-World Example**:

```
Disease Screening:
TP = 40 (correctly identified sick patients)
FN = 10 (missed sick patients)

Recall = 40 / (40 + 10) = 0.80 = 80%

Meaning: We catch 80% of sick patients
Problem: 20% of sick patients are missed (false negatives!)
```

**When to Prioritize Recall**:
- Disease diagnosis (don't miss sick patients)
- Fraud detection (catch all fraudulent transactions)
- Security systems (detect all threats)
- Search engines (find all relevant documents)

#### 3.4 F1-Score

**Definition**: Harmonic mean of precision and recall

```
F1 = 2 Ã— (Precision Ã— Recall) / (Precision + Recall)
```

**Why Harmonic Mean?**
- Penalizes extreme imbalance
- Both precision and recall must be high
- Better than arithmetic mean for rates

**Code Example**:

```csharp
double f1 = ClassificationMetrics.F1Score(yTrue, yPred, positiveClass: 1);
Console.WriteLine($"F1-Score: {f1:P2}");
// Output: F1-Score: 81.63%
```

**Comparison of Means**:

```
Scenario 1: Balanced
Precision = 0.8, Recall = 0.8
Arithmetic Mean = (0.8 + 0.8) / 2 = 0.80
Harmonic Mean (F1) = 2Ã—(0.8Ã—0.8)/(0.8+0.8) = 0.80
âœ… Both agree

Scenario 2: Imbalanced
Precision = 0.9, Recall = 0.1
Arithmetic Mean = (0.9 + 0.1) / 2 = 0.50  â† Misleading!
Harmonic Mean (F1) = 2Ã—(0.9Ã—0.1)/(0.9+0.1) = 0.18  â† Realistic!
âœ… F1 reveals poor performance
```

**When to Use F1-Score**:
- Imbalanced datasets
- Need balance between precision and recall
- Single metric for model comparison
- General classification tasks

#### 3.5 Complete Classification Evaluation Example

**Problem**: Evaluate a spam detection model

```csharp
using ArtificialIntelligence.MachineLearning.Supervised.Classification;
using ArtificialIntelligence.MachineLearning.Supervised.Evaluation;

public class ClassificationEvaluationExample
{
    public static void Main()
    {
        // Step 1: Prepare training data
        // Features: [spam_words, links, exclamations]
        double[,] XTrain = new double[,] {
            { 0, 0, 0 },  // Normal
            { 1, 0, 1 },  // Normal
            { 5, 3, 5 },  // Spam
            { 8, 5, 8 },  // Spam
            { 0, 1, 0 },  // Normal
            { 10, 8, 10 } // Spam
        };

        int[] yTrain = new int[] { 0, 0, 1, 1, 0, 1 }; // 0=Normal, 1=Spam

        // Step 2: Train model
        var model = new LogisticRegression(learningRate: 0.1, maxIterations: 1000);
        model.Fit(XTrain, yTrain);

        // Step 3: Prepare test data
        double[,] XTest = new double[,] {
            { 0, 0, 0 },  // Normal
            { 6, 4, 6 },  // Spam
            { 1, 1, 1 },  // Normal
            { 9, 7, 9 },  // Spam
            { 0, 0, 1 },  // Normal
            { 7, 5, 7 }   // Spam
        };

        int[] yTest = new int[] { 0, 1, 0, 1, 0, 1 };

        // Step 4: Make predictions
        int[] yPred = model.Predict(XTest);

        // Step 5: Calculate all metrics
        double accuracy = ClassificationMetrics.Accuracy(yTest, yPred);
        double precision = ClassificationMetrics.Precision(yTest, yPred, positiveClass: 1);
        double recall = ClassificationMetrics.Recall(yTest, yPred, positiveClass: 1);
        double f1 = ClassificationMetrics.F1Score(yTest, yPred, positiveClass: 1);
        double specificity = ClassificationMetrics.Specificity(yTest, yPred, positiveClass: 1);

        // Step 6: Display evaluation report
        Console.WriteLine("=== Classification Model Evaluation Report ===\n");
        Console.WriteLine($"Accuracy:    {accuracy:P2}");
        Console.WriteLine($"Precision:   {precision:P2}");
        Console.WriteLine($"Recall:      {recall:P2}");
        Console.WriteLine($"F1-Score:    {f1:P2}");
        Console.WriteLine($"Specificity: {specificity:P2}");

        // Step 7: Interpret results
        Console.WriteLine("\n=== Interpretation ===");

        if (accuracy > 0.9)
            Console.WriteLine("âœ… Excellent overall accuracy");
        else if (accuracy > 0.8)
            Console.WriteLine("âœ… Good overall accuracy");
        else
            Console.WriteLine("âŒ Poor overall accuracy");

        if (precision > 0.9)
            Console.WriteLine("âœ… Very few false alarms (legitimate emails marked as spam)");
        else if (precision < 0.7)
            Console.WriteLine("âš ï¸ Warning: Many false alarms");

        if (recall > 0.9)
            Console.WriteLine("âœ… Catching most spam emails");
        else if (recall < 0.7)
            Console.WriteLine("âš ï¸ Warning: Missing many spam emails");

        if (f1 > 0.85)
            Console.WriteLine("âœ… Good balance between precision and recall");
    }
}
```

#### 3.6 Metric Selection Guide

**Quick Decision Tree**:

```
Is data balanced?
â”œâ”€ Yes â†’ Use Accuracy
â””â”€ No â†’ What's more important?
    â”œâ”€ Avoid false alarms â†’ Use Precision
    â”œâ”€ Catch all positives â†’ Use Recall
    â””â”€ Balance both â†’ Use F1-Score
```

**Practical Examples**:

| Scenario | Primary Metric | Reason |
|----------|---------------|--------|
| **Email spam filter** | Precision | Don't block legitimate emails |
| **Cancer screening** | Recall | Don't miss sick patients |
| **Fraud detection** | Recall | Catch all fraudulent transactions |
| **Product recommendations** | Precision | Don't annoy users with bad suggestions |
| **Customer churn** | F1-Score | Balance between catching churners and avoiding false alarms |
| **Credit approval** | F1-Score | Balance risk and opportunity |

---

### Chapter 4: The Train-Test Split

#### 4.1 Why Split Data?

**The Fundamental Problem**:
```
If you test on training data:
- Model has "seen" the answers
- Like taking the same exam you studied from
- Results are overly optimistic
- Doesn't measure generalization

If you test on new data:
- Model hasn't "seen" the answers
- Like taking a different exam
- Results are realistic
- Measures true generalization
```

#### 4.2 The Three-Way Split

**Standard Practice**: Split data into three sets

```
Total Data (100%)
â”œâ”€ Training Set (60-80%)
â”‚  â””â”€ Used to train the model
â”œâ”€ Validation Set (10-20%)
â”‚  â””â”€ Used to tune hyperparameters
â””â”€ Test Set (10-20%)
   â””â”€ Used for final evaluation (ONLY ONCE!)
```

**Detailed Explanation**:

**1. Training Set**:
- **Purpose**: Learn model parameters
- **Size**: 60-80% of data
- **Usage**: Model sees this during training
- **Can use**: Multiple times during training

**2. Validation Set**:
- **Purpose**: Tune hyperparameters, select models
- **Size**: 10-20% of data
- **Usage**: Evaluate different configurations
- **Can use**: Multiple times during development

**3. Test Set**:
- **Purpose**: Final performance evaluation
- **Size**: 10-20% of data
- **Usage**: Evaluate final model
- **Can use**: ONLY ONCE at the very end!

#### 4.3 Implementation

**Simple Train-Test Split (80/20)**:

```csharp
public static (double[,], double[], double[,], double[])
    TrainTestSplit(double[,] X, double[] y, double testSize = 0.2, int? randomSeed = null)
{
    int n = y.Length;
    int m = X.GetLength(1);
    int testCount = (int)(n * testSize);
    int trainCount = n - testCount;

    // Set random seed for reproducibility
    Random random = randomSeed.HasValue ? new Random(randomSeed.Value) : new Random();

    // Shuffle indices
    var indices = Enumerable.Range(0, n).OrderBy(x => random.Next()).ToArray();

    // Allocate arrays
    double[,] XTrain = new double[trainCount, m];
    double[] yTrain = new double[trainCount];
    double[,] XTest = new double[testCount, m];
    double[] yTest = new double[testCount];

    // Fill training set
    for (int i = 0; i < trainCount; i++)
    {
        int idx = indices[i];
        for (int j = 0; j < m; j++)
            XTrain[i, j] = X[idx, j];
        yTrain[i] = y[idx];
    }

    // Fill test set
    for (int i = 0; i < testCount; i++)
    {
        int idx = indices[trainCount + i];
        for (int j = 0; j < m; j++)
            XTest[i, j] = X[idx, j];
        yTest[i] = y[idx];
    }

    return (XTrain, yTrain, XTest, yTest);
}
```

**Three-Way Split (70/15/15)**:

```csharp
public static (double[,], double[], double[,], double[], double[,], double[])
    TrainValTestSplit(double[,] X, double[] y,
                      double trainRatio = 0.7,
                      double valRatio = 0.15,
                      int? randomSeed = null)
{
    int n = y.Length;
    int m = X.GetLength(1);

    int trainSize = (int)(n * trainRatio);
    int valSize = (int)(n * valRatio);
    int testSize = n - trainSize - valSize;

    // Shuffle
    Random random = randomSeed.HasValue ? new Random(randomSeed.Value) : new Random();
    var indices = Enumerable.Range(0, n).OrderBy(x => random.Next()).ToArray();

    // Allocate arrays
    double[,] XTrain = new double[trainSize, m];
    double[] yTrain = new double[trainSize];
    double[,] XVal = new double[valSize, m];
    double[] yVal = new double[valSize];
    double[,] XTest = new double[testSize, m];
    double[] yTest = new double[testSize];

    // Fill training set
    for (int i = 0; i < trainSize; i++)
    {
        int idx = indices[i];
        for (int j = 0; j < m; j++)
            XTrain[i, j] = X[idx, j];
        yTrain[i] = y[idx];
    }

    // Fill validation set
    for (int i = 0; i < valSize; i++)
    {
        int idx = indices[trainSize + i];
        for (int j = 0; j < m; j++)
            XVal[i, j] = X[idx, j];
        yVal[i] = y[idx];
    }

    // Fill test set
    for (int i = 0; i < testSize; i++)
    {
        int idx = indices[trainSize + valSize + i];
        for (int j = 0; j < m; j++)
            XTest[i, j] = X[idx, j];
        yTest[i] = y[idx];
    }

    return (XTrain, yTrain, XVal, yVal, XTest, yTest);
}
```

#### 4.4 Best Practices

**1. Always Shuffle Before Splitting**:
```csharp
// âŒ BAD: Don't split sequentially
// First 80% for training, last 20% for testing
// Problem: May have temporal or ordering bias

// âœ… GOOD: Shuffle first
var indices = Enumerable.Range(0, n).OrderBy(x => Random.Shared.Next()).ToArray();
```

**2. Use Random Seed for Reproducibility**:
```csharp
// âœ… GOOD: Set seed for reproducible results
var (XTrain, yTrain, XTest, yTest) = TrainTestSplit(X, y, testSize: 0.2, randomSeed: 42);
// Same split every time you run the code
```

**3. Stratified Splitting for Imbalanced Data**:
```csharp
// For classification with imbalanced classes
// Ensure each split has same class distribution
// Example: If 90% class 0, 10% class 1
// Each split should maintain this ratio
```

**4. Never Touch Test Set Until Final Evaluation**:
```
Development Phase:
â”œâ”€ Train on training set
â”œâ”€ Evaluate on validation set
â”œâ”€ Tune hyperparameters
â”œâ”€ Select best model
â””â”€ Repeat as needed

Final Phase:
â””â”€ Evaluate on test set (ONCE!)
```

#### 4.5 Common Pitfalls

**Pitfall 1: Data Leakage**:
```csharp
// âŒ BAD: Feature scaling before split
XScaled = StandardizeFeatures(X);  // Uses statistics from ALL data!
(XTrain, XTest) = Split(XScaled);

// âœ… GOOD: Split first, then scale
(XTrain, XTest) = Split(X);
XTrain = StandardizeFeatures(XTrain);
XTest = StandardizeFeatures(XTest);  // Use training statistics
```

**Pitfall 2: Using Test Set Multiple Times**:
```
âŒ BAD Workflow:
1. Train model A, test on test set â†’ 85%
2. Train model B, test on test set â†’ 87%
3. Train model C, test on test set â†’ 90%
4. Choose model C

Problem: You've "optimized" for the test set!
Test set is no longer independent!

âœ… GOOD Workflow:
1. Train model A, test on validation set â†’ 85%
2. Train model B, test on validation set â†’ 87%
3. Train model C, test on validation set â†’ 90%
4. Choose model C
5. Final evaluation on test set (once!) â†’ 89%
```

**Pitfall 3: Too Small Test Set**:
```
Dataset: 100 samples
Test set: 10 samples (10%)

Problem: High variance in evaluation
One outlier can change results significantly

Solution: Use cross-validation (covered in Chapter 9)
```

---

## Part II: Intermediate Level

### Chapter 5: Understanding RÂ² (Coefficient of Determination)

#### 5.1 What is RÂ²?

**Definition**: RÂ² measures the proportion of variance in the target variable that is explained by the model.

```
RÂ² = 1 - (SS_res / SS_tot)

Where:
SS_res = Î£(yáµ¢ - Å·áµ¢)Â²  (Residual Sum of Squares)
SS_tot = Î£(yáµ¢ - È³)Â²   (Total Sum of Squares)
È³ = mean of actual values
```

**Intuitive Explanation**:
- **SS_tot**: Total variance in data (how spread out the data is)
- **SS_res**: Variance not explained by model (errors)
- **RÂ²**: Proportion of variance explained by model

#### 5.2 Interpreting RÂ² Values

| RÂ² Value | Interpretation | Model Quality |
|----------|---------------|---------------|
| **1.0** | Perfect fit | Ideal (rarely achieved) |
| **0.9-1.0** | Excellent | Very strong relationship |
| **0.7-0.9** | Good | Strong relationship |
| **0.5-0.7** | Moderate | Moderate relationship |
| **0.3-0.5** | Weak | Weak relationship |
| **< 0.3** | Very weak | Poor model |
| **< 0** | Worse than mean | Model is useless |

**Code Example**:

```csharp
using ArtificialIntelligence.MachineLearning.Supervised.Evaluation;

double[] yTrue = new double[] { 100, 200, 300, 400, 500 };
double[] yPred = new double[] { 110, 190, 310, 390, 510 };

double r2 = RegressionMetrics.RSquared(yTrue, yPred);
Console.WriteLine($"RÂ²: {r2:F4}");
Console.WriteLine($"Model explains {r2:P2} of the variance");

// Output:
// RÂ²: 0.9800
// Model explains 98.00% of the variance
```

#### 5.3 RÂ² vs Adjusted RÂ²

**Problem with RÂ²**: Always increases when adding features, even if they're useless!

**Adjusted RÂ²**: Penalizes adding unnecessary features

```
Adjusted RÂ² = 1 - [(1-RÂ²) Ã— (n-1) / (n-p-1)]

Where:
n = number of samples
p = number of features
```

**When to Use**:
- **RÂ²**: Comparing models with same number of features
- **Adjusted RÂ²**: Comparing models with different number of features

#### 5.4 Limitations of RÂ²

**Limitation 1: Doesn't indicate if model is appropriate**
```
High RÂ² doesn't mean:
- Model is correct
- Predictions are accurate
- Relationship is causal
```

**Limitation 2: Sensitive to outliers**
```
One extreme outlier can drastically change RÂ²
```

**Limitation 3: Can be negative**
```
RÂ² < 0 means: Model is worse than predicting the mean!
```

**Code Example - Understanding RÂ²**:

```csharp
public class RSquaredExample
{
    public static void Main()
    {
        // Scenario 1: Perfect fit
        double[] yTrue1 = new double[] { 1, 2, 3, 4, 5 };
        double[] yPred1 = new double[] { 1, 2, 3, 4, 5 };
        double r2_1 = RegressionMetrics.RSquared(yTrue1, yPred1);
        Console.WriteLine($"Perfect fit: RÂ² = {r2_1:F4}");  // 1.0000

        // Scenario 2: Good fit
        double[] yTrue2 = new double[] { 1, 2, 3, 4, 5 };
        double[] yPred2 = new double[] { 1.1, 1.9, 3.1, 3.9, 5.1 };
        double r2_2 = RegressionMetrics.RSquared(yTrue2, yPred2);
        Console.WriteLine($"Good fit: RÂ² = {r2_2:F4}");  // ~0.98

        // Scenario 3: Predicting mean (baseline)
        double[] yTrue3 = new double[] { 1, 2, 3, 4, 5 };
        double mean = yTrue3.Average();
        double[] yPred3 = Enumerable.Repeat(mean, 5).ToArray();
        double r2_3 = RegressionMetrics.RSquared(yTrue3, yPred3);
        Console.WriteLine($"Predicting mean: RÂ² = {r2_3:F4}");  // 0.0000

        // Scenario 4: Worse than mean
        double[] yTrue4 = new double[] { 1, 2, 3, 4, 5 };
        double[] yPred4 = new double[] { 5, 4, 3, 2, 1 };  // Opposite!
        double r2_4 = RegressionMetrics.RSquared(yTrue4, yPred4);
        Console.WriteLine($"Worse than mean: RÂ² = {r2_4:F4}");  // Negative!
    }
}
```

---

### Chapter 6: The Confusion Matrix

#### 6.1 Understanding the Matrix

**Structure for Binary Classification**:

```
                    Predicted
                 Negative  Positive
Actual  Negative    TN        FP
        Positive    FN        TP
```

**Definitions**:
- **TP (True Positive)**: Correctly predicted positive
- **TN (True Negative)**: Correctly predicted negative
- **FP (False Positive)**: Incorrectly predicted positive (Type I Error)
- **FN (False Negative)**: Incorrectly predicted negative (Type II Error)

#### 6.2 Creating a Confusion Matrix

**Code Example**:

```csharp
using ArtificialIntelligence.MachineLearning.Supervised.Evaluation;

// Predictions vs actual labels
int[] yTrue = new int[] { 0, 1, 1, 0, 1, 0, 1, 1, 0, 0 };
int[] yPred = new int[] { 0, 1, 0, 0, 1, 1, 1, 1, 0, 1 };

// Create confusion matrix
var cm = new ConfusionMatrix(yTrue, yPred);

// Display matrix
Console.WriteLine("Confusion Matrix:");
Console.WriteLine(cm.ToString());

// Get individual values
int tp = cm.GetTruePositives(1);
int tn = cm.GetTrueNegatives(1);
int fp = cm.GetFalsePositives(1);
int fn = cm.GetFalseNegatives(1);

Console.WriteLine($"\nBreakdown:");
Console.WriteLine($"True Positives (TP):  {tp}");
Console.WriteLine($"True Negatives (TN):  {tn}");
Console.WriteLine($"False Positives (FP): {fp}");
Console.WriteLine($"False Negatives (FN): {fn}");

// Calculate metrics from confusion matrix
double precision = (double)tp / (tp + fp);
double recall = (double)tp / (tp + fn);
double accuracy = (double)(tp + tn) / (tp + tn + fp + fn);

Console.WriteLine($"\nMetrics:");
Console.WriteLine($"Precision: {precision:P2}");
Console.WriteLine($"Recall:    {recall:P2}");
Console.WriteLine($"Accuracy:  {accuracy:P2}");
```

#### 6.3 Multi-class Confusion Matrix

**Example: 3-class Classification**

```
                    Predicted
                 Class 0  Class 1  Class 2
Actual  Class 0    50       3        2
        Class 1     5      45        4
        Class 2     1       2       48
```

**Reading the Matrix**:
- **Diagonal**: Correct predictions (50, 45, 48)
- **Off-diagonal**: Errors
- **Row sums**: Actual class counts
- **Column sums**: Predicted class counts

**Code Example**:

```csharp
// Multi-class example: Iris classification
int[] yTrue = new int[] { 0, 0, 0, 1, 1, 1, 2, 2, 2, 0, 1, 2 };
int[] yPred = new int[] { 0, 0, 1, 1, 1, 2, 2, 2, 1, 0, 1, 2 };

var cm = new ConfusionMatrix(yTrue, yPred);
Console.WriteLine(cm.ToString());

// Per-class metrics
for (int classLabel = 0; classLabel < 3; classLabel++)
{
    int tp = cm.GetTruePositives(classLabel);
    int fp = cm.GetFalsePositives(classLabel);
    int fn = cm.GetFalseNegatives(classLabel);

    double precision = (double)tp / (tp + fp);
    double recall = (double)tp / (tp + fn);

    Console.WriteLine($"\nClass {classLabel}:");
    Console.WriteLine($"  Precision: {precision:P2}");
    Console.WriteLine($"  Recall:    {recall:P2}");
}
```

#### 6.4 Analyzing Confusion Patterns

**Pattern 1: Perfect Classification**
```
        Pred 0  Pred 1
Act 0    100      0
Act 1      0    100

âœ… All predictions on diagonal
âœ… No errors
```

**Pattern 2: Systematic Bias**
```
        Pred 0  Pred 1
Act 0     90     10
Act 1     40     60

âŒ Model biased toward Class 0
âŒ Many Class 1 samples misclassified as Class 0
```

**Pattern 3: Class Confusion**
```
        Pred 0  Pred 1  Pred 2
Act 0     50      0      0
Act 1      5     40      5
Act 2      0      8     42

âŒ Classes 1 and 2 often confused with each other
âœ… Class 0 well separated
```

#### 6.5 Using Confusion Matrix for Improvement

**Step 1: Identify Problem Areas**
```csharp
public static void AnalyzeConfusionMatrix(ConfusionMatrix cm, int numClasses)
{
    Console.WriteLine("=== Confusion Matrix Analysis ===\n");

    for (int i = 0; i < numClasses; i++)
    {
        int tp = cm.GetTruePositives(i);
        int fn = cm.GetFalseNegatives(i);
        int fp = cm.GetFalsePositives(i);

        double recall = (double)tp / (tp + fn);
        double precision = (double)tp / (tp + fp);

        Console.WriteLine($"Class {i}:");

        if (recall < 0.7)
            Console.WriteLine($"  âš ï¸ Low recall ({recall:P2}) - Many missed predictions");

        if (precision < 0.7)
            Console.WriteLine($"  âš ï¸ Low precision ({precision:P2}) - Many false alarms");

        if (recall > 0.9 && precision > 0.9)
            Console.WriteLine($"  âœ… Excellent performance");
    }
}
```

**Step 2: Take Action**
- **Low recall**: Model missing many positives â†’ Add more positive examples, adjust threshold
- **Low precision**: Model has many false alarms â†’ Improve features, increase threshold
- **Class confusion**: Two classes often confused â†’ Add distinguishing features

---

### Chapter 7: Precision-Recall Tradeoff

#### 7.1 The Fundamental Tradeoff

**The Dilemma**: You cannot maximize both precision and recall simultaneously.

```
High Precision â†’ Low Recall
(Conservative predictions)

High Recall â†’ Low Precision
(Aggressive predictions)
```

**Why the Tradeoff Exists**:
- To increase precision: Be more selective (higher threshold)
- To increase recall: Be more inclusive (lower threshold)

#### 7.2 Threshold Adjustment

**Default Threshold**: 0.5
```
If P(positive) â‰¥ 0.5 â†’ Predict positive
If P(positive) < 0.5 â†’ Predict negative
```

**Adjusting Threshold**:

```csharp
public static int[] PredictWithThreshold(double[] probabilities, double threshold)
{
    return probabilities.Select(p => p >= threshold ? 1 : 0).ToArray();
}

// Example: Try different thresholds
var model = new LogisticRegression();
model.Fit(XTrain, yTrain);

double[] probabilities = model.PredictProba(XTest);
double[] thresholds = { 0.3, 0.5, 0.7, 0.9 };

Console.WriteLine("Threshold\tPrecision\tRecall\t\tF1-Score");
Console.WriteLine("--------------------------------------------------------");

foreach (var threshold in thresholds)
{
    int[] predictions = PredictWithThreshold(probabilities, threshold);

    double precision = ClassificationMetrics.Precision(yTest, predictions, 1);
    double recall = ClassificationMetrics.Recall(yTest, predictions, 1);
    double f1 = ClassificationMetrics.F1Score(yTest, predictions, 1);

    Console.WriteLine($"{threshold:F1}\t\t{precision:P2}\t\t{recall:P2}\t\t{f1:P2}");
}
```

**Expected Output**:
```
Threshold	Precision	Recall		F1-Score
--------------------------------------------------------
0.3		65.00%		95.00%		77.11%
0.5		80.00%		80.00%		80.00%
0.7		90.00%		60.00%		72.00%
0.9		95.00%		40.00%		56.47%
```

#### 7.3 Choosing the Right Threshold

**Scenario-Based Selection**:

| Scenario | Threshold | Reason |
|----------|-----------|--------|
| **Spam filter** | High (0.7-0.9) | Avoid blocking legitimate emails |
| **Disease screening** | Low (0.2-0.4) | Don't miss sick patients |
| **Fraud detection** | Low (0.3-0.5) | Catch all fraudulent transactions |
| **Balanced** | Medium (0.5) | Equal weight to both metrics |

#### 7.4 Precision-Recall Curve

**Concept**: Plot precision vs recall for all possible thresholds

**Code Example**:

```csharp
public static void PlotPrecisionRecallCurve(double[] yTrue, double[] probabilities)
{
    // Generate thresholds from 0 to 1
    double[] thresholds = Enumerable.Range(0, 101)
        .Select(i => i / 100.0)
        .ToArray();

    Console.WriteLine("Threshold\tPrecision\tRecall");
    Console.WriteLine("----------------------------------------");

    foreach (var threshold in thresholds)
    {
        int[] predictions = probabilities
            .Select(p => p >= threshold ? 1 : 0)
            .ToArray();

        double precision = ClassificationMetrics.Precision(
            yTrue.Select(y => (int)y).ToArray(),
            predictions,
            1
        );
        double recall = ClassificationMetrics.Recall(
            yTrue.Select(y => (int)y).ToArray(),
            predictions,
            1
        );

        Console.WriteLine($"{threshold:F2}\t\t{precision:P2}\t\t{recall:P2}");
    }
}
```

---

### Chapter 8: ROC Curves and AUC

#### 8.1 Understanding ROC Curves

**ROC (Receiver Operating Characteristic)**: Plot of True Positive Rate vs False Positive Rate

```
TPR (True Positive Rate) = Recall = TP / (TP + FN)
FPR (False Positive Rate) = FP / (FP + TN)
```

**Visual Representation**:
```
TPR â†‘
1.0 â”‚    â•±â”€â”€â”€â”€
    â”‚   â•±
    â”‚  â•±  â† Good model
    â”‚ â•±
0.5 â”‚â•±   â† Random guessing
    â”‚
0.0 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ FPR
   0.0   0.5   1.0
```

#### 8.2 AUC (Area Under Curve)

**Definition**: Area under the ROC curve

**Interpretation**:
- **AUC = 1.0**: Perfect classifier
- **AUC = 0.9-1.0**: Excellent
- **AUC = 0.8-0.9**: Good
- **AUC = 0.7-0.8**: Fair
- **AUC = 0.5-0.7**: Poor
- **AUC = 0.5**: Random guessing
- **AUC < 0.5**: Worse than random (predictions are inverted!)

**Code Example**:

```csharp
public static double CalculateAUC(int[] yTrue, double[] probabilities)
{
    // Sort by probability (descending)
    var sorted = yTrue.Zip(probabilities, (y, p) => new { y, p })
        .OrderByDescending(x => x.p)
        .ToArray();

    int positives = yTrue.Count(y => y == 1);
    int negatives = yTrue.Length - positives;

    double auc = 0;
    int truePositives = 0;

    foreach (var item in sorted)
    {
        if (item.y == 1)
        {
            truePositives++;
        }
        else
        {
            // For each negative, count how many positives are ranked higher
            auc += truePositives;
        }
    }

    return auc / (positives * negatives);
}
```

#### 8.3 ROC vs Precision-Recall

**When to Use ROC-AUC**:
- Balanced datasets
- Care about both classes equally
- Standard metric for comparison

**When to Use Precision-Recall**:
- Imbalanced datasets
- Care more about positive class
- Fraud detection, disease diagnosis

**Example**:
```
Dataset: 95% negative, 5% positive

ROC-AUC: May look good (0.85)
But: Model might catch only 50% of positives

Precision-Recall: Shows true performance
F1-Score: 0.60 (reveals poor performance on minority class)
```

---

## Part III: Advanced Level

### Chapter 9: Cross-Validation Techniques

#### 9.1 Why Cross-Validation?

**Problem with Single Split**:
- Results depend on which samples end up in test set
- May get lucky or unlucky split
- High variance in performance estimate

**Solution**: Cross-validation
- Multiple train-test splits
- Average results across splits
- More robust performance estimate

#### 9.2 K-Fold Cross-Validation

**Process**:
1. Split data into K equal folds
2. For each fold:
   - Use fold as test set
   - Use remaining K-1 folds as training set
   - Train and evaluate
3. Average K evaluation scores

**Code Implementation**:

```csharp
public static double KFoldCrossValidation(double[,] X, double[] y, int k = 5)
{
    int n = y.Length;
    int foldSize = n / k;
    double totalScore = 0;

    Console.WriteLine($"Performing {k}-Fold Cross-Validation\n");

    for (int fold = 0; fold < k; fold++)
    {
        // Create train-test split for this fold
        int testStart = fold * foldSize;
        int testEnd = (fold == k - 1) ? n : testStart + foldSize;

        var (XTrain, yTrain, XTest, yTest) = CreateFold(X, y, testStart, testEnd);

        // Train model
        var model = new LinearRegression();
        model.Fit(XTrain, yTrain);

        // Evaluate
        double[] yPred = model.Predict(XTest);
        double r2 = RegressionMetrics.RSquared(yTest, yPred);

        totalScore += r2;
        Console.WriteLine($"Fold {fold + 1}: RÂ² = {r2:F4}");
    }

    double avgScore = totalScore / k;
    Console.WriteLine($"\nAverage RÂ²: {avgScore:F4}");
    Console.WriteLine($"Std Dev: {CalculateStdDev(scores):F4}");

    return avgScore;
}
```

#### 9.3 Stratified K-Fold

**For Classification**: Maintain class distribution in each fold

```csharp
public static double StratifiedKFold(double[,] X, int[] y, int k = 5)
{
    // Ensure each fold has same proportion of each class
    // Important for imbalanced datasets

    var classCounts = y.GroupBy(label => label)
        .ToDictionary(g => g.Key, g => g.Count());

    // Implementation details...
    // Split each class separately, then combine
}
```

#### 9.4 Leave-One-Out Cross-Validation (LOOCV)

**Extreme case**: K = n (number of samples)
- Each sample is test set once
- Maximum use of data
- Very computationally expensive

**When to Use**:
- Very small datasets (< 100 samples)
- When you can't afford to lose training data

---

### Chapter 10: Learning Curves

#### 10.1 What are Learning Curves?

**Definition**: Plot of model performance vs training set size

**Purpose**:
- Diagnose overfitting/underfitting
- Determine if more data would help
- Guide model selection

#### 10.2 Interpreting Learning Curves

**Pattern 1: High Bias (Underfitting)**
```
Score â†‘
      â”‚ â”€â”€â”€â”€â”€â”€ Training score (low)
      â”‚ â”€â”€â”€â”€â”€â”€ Validation score (low)
      â”‚ (Both converge to low value)
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ Training size

Diagnosis: Model too simple
Solution: Use more complex model, add features
```

**Pattern 2: High Variance (Overfitting)**
```
Score â†‘
      â”‚ â”€â”€â”€â”€â”€â”€ Training score (high)
      â”‚     â•²
      â”‚      â•²â”€â”€ Validation score (low)
      â”‚ (Large gap between curves)
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ Training size

Diagnosis: Model too complex
Solution: More data, regularization, simpler model
```

**Pattern 3: Good Fit**
```
Score â†‘
      â”‚ â”€â”€â”€â”€â”€â”€ Training score
      â”‚ â”€â”€â”€â”€â”€â”€ Validation score
      â”‚ (Both high and close together)
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ Training size

Diagnosis: Model is well-tuned
Solution: Deploy!
```

#### 10.3 Implementation

```csharp
public static void PlotLearningCurve(double[,] X, double[] y)
{
    int n = y.Length;
    int[] trainSizes = {
        n / 10, n / 5, n / 3, n / 2,
        (int)(n * 0.7), (int)(n * 0.9)
    };

    Console.WriteLine("Training Size\tTrain RÂ²\tVal RÂ²\t\tGap");
    Console.WriteLine("--------------------------------------------------------");

    foreach (var size in trainSizes)
    {
        // Use first 'size' samples for training
        var (XTrain, yTrain, XVal, yVal) = SplitBySize(X, y, size);

        var model = new LinearRegression();
        model.Fit(XTrain, yTrain);

        // Evaluate on both sets
        double[] yTrainPred = model.Predict(XTrain);
        double[] yValPred = model.Predict(XVal);

        double trainR2 = RegressionMetrics.RSquared(yTrain, yTrainPred);
        double valR2 = RegressionMetrics.RSquared(yVal, yValPred);
        double gap = trainR2 - valR2;

        Console.WriteLine($"{size}\t\t{trainR2:F4}\t\t{valR2:F4}\t\t{gap:F4}");

        // Diagnosis
        if (gap > 0.2)
            Console.WriteLine("  âš ï¸ High variance - Consider more data or regularization");
        else if (trainR2 < 0.7 && valR2 < 0.7)
            Console.WriteLine("  âš ï¸ High bias - Consider more complex model");
    }
}
```

---

### Chapter 11: Model Selection and Comparison

#### 11.1 Comparing Multiple Models

**Framework**:

```csharp
public static void CompareModels(double[,] X, int[] y)
{
    var models = new Dictionary<string, dynamic>
    {
        { "Logistic Regression", new LogisticRegression() },
        { "KNN (k=3)", new KNearestNeighbors(k: 3) },
        { "Decision Tree", new DecisionTreeClassifier(maxDepth: 5) },
        { "Naive Bayes", new NaiveBayesClassifier() }
    };

    var (XTrain, yTrain, XTest, yTest) = TrainTestSplit(X, y, 0.2);

    Console.WriteLine($"{"Model",-25} {"Accuracy",-12} {"F1-Score",-12} {"Time (ms)",-12}");
    Console.WriteLine(new string('-', 61));

    foreach (var (name, model) in models)
    {
        var sw = System.Diagnostics.Stopwatch.StartNew();

        model.Fit(XTrain, yTrain);
        int[] yPred = model.Predict(XTest);

        sw.Stop();

        double accuracy = ClassificationMetrics.Accuracy(yTest, yPred);
        double f1 = ClassificationMetrics.F1Score(yTest, yPred, 1);

        Console.WriteLine($"{name,-25} {accuracy,-12:P2} {f1,-12:P2} {sw.ElapsedMilliseconds,-12}");
    }
}
```

#### 11.2 Statistical Significance Testing

**Question**: Is model A really better than model B, or just lucky?

**Approach**: Use cross-validation with statistical tests

```csharp
public static bool IsSignificantlyBetter(double[] scoresA, double[] scoresB, double alpha = 0.05)
{
    // Paired t-test
    double[] differences = scoresA.Zip(scoresB, (a, b) => a - b).ToArray();
    double meanDiff = differences.Average();
    double stdDiff = CalculateStdDev(differences);
    double tStat = meanDiff / (stdDiff / Math.Sqrt(differences.Length));

    // Compare to critical value
    // (Simplified - use proper t-distribution in production)
    return Math.Abs(tStat) > 2.0;  // Approximate for alpha=0.05
}
```

---

### Chapter 12: Production Monitoring

#### 12.1 Why Monitor Models?

**Models degrade over time**:
- Data distribution changes
- Relationships change
- New patterns emerge

**Example**:
```
Model trained in 2024:
- Accuracy: 90%

Same model in 2026:
- Accuracy: 70%

Why? Customer behavior changed!
```

#### 12.2 Key Metrics to Monitor

**1. Performance Metrics**:
```csharp
public class ModelMonitor
{
    private Queue<double> _recentAccuracy = new Queue<double>();
    private const int WindowSize = 1000;

    public void LogPrediction(int prediction, int actual)
    {
        bool correct = prediction == actual;
        _recentAccuracy.Enqueue(correct ? 1.0 : 0.0);

        if (_recentAccuracy.Count > WindowSize)
            _recentAccuracy.Dequeue();

        double rollingAccuracy = _recentAccuracy.Average();

        if (rollingAccuracy < 0.7)
        {
            Console.WriteLine($"âš ï¸ ALERT: Accuracy dropped to {rollingAccuracy:P2}");
            Console.WriteLine("Consider retraining the model!");
        }
    }
}
```

**2. Prediction Distribution**:
```csharp
// Monitor if prediction distribution changes
public void MonitorPredictionDistribution(int[] predictions)
{
    var distribution = predictions.GroupBy(p => p)
        .ToDictionary(g => g.Key, g => g.Count() / (double)predictions.Length);

    // Compare to expected distribution
    // Alert if significant drift
}
```

**3. Feature Distribution**:
```csharp
// Monitor if input features change
public void MonitorFeatureDistribution(double[,] features)
{
    // Calculate statistics for each feature
    // Compare to training distribution
    // Alert if significant drift
}
```

#### 12.3 When to Retrain

**Triggers for Retraining**:
- Performance drops below threshold
- Significant data drift detected
- New data patterns emerge
- Regular schedule (e.g., monthly)

---

## Conclusion

Congratulations! You've mastered model evaluation from beginner to expert level.

### Key Takeaways

**1. Always Use Proper Evaluation**:
- Never evaluate on training data
- Use appropriate metrics for your problem
- Consider multiple metrics

**2. Choose Right Metrics**:
- Regression: RMSE for interpretability, RÂ² for variance explained
- Classification: F1-Score for imbalanced data, Accuracy for balanced
- Consider business context

**3. Robust Validation**:
- Use cross-validation for small datasets
- Use learning curves to diagnose problems
- Compare multiple models statistically

**4. Production Monitoring**:
- Monitor performance continuously
- Detect data drift
- Retrain when necessary

### Metric Selection Summary

| Problem Type | Recommended Metrics | When to Use |
|--------------|-------------------|-------------|
| **Regression** | RMSE, RÂ² | Standard regression tasks |
| **Balanced Classification** | Accuracy, F1-Score | Equal class importance |
| **Imbalanced Classification** | Precision, Recall, F1-Score | Fraud, disease detection |
| **Ranking** | AUC-ROC | Model comparison |

### Next Steps

1. **Practice**: Apply evaluation to your own projects
2. **Experiment**: Try different metrics and validation strategies
3. **Learn More**: Study advanced topics like Bayesian optimization
4. **Build**: Create production monitoring systems

### Resources for .NET/C# Machine Learning

- ML.NET Documentation (Microsoft's ML framework)
- Accord.NET Framework (comprehensive ML library)
- Math.NET Numerics (numerical computing)
- "Programming ML.NET" by Matt R. Cole

---

## Alternative Technology Stack

**Note**: While this tutorial uses the **.NET C# technology stack**, the evaluation concepts and techniques are universal and apply to any programming language.

**Python Alternative**: Python offers excellent evaluation tools:
- **scikit-learn**: Comprehensive metrics and validation
- **Matplotlib/Seaborn**: Visualization of curves and matrices
- **Pandas**: Data manipulation for analysis

Both ecosystems provide robust tools for model evaluation.

---

**Happy Evaluating!** ğŸ“Š

*Last Updated: 2026-01-29*

